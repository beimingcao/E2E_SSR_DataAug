{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "927fa460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beiming/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import yaml\n",
    "import os\n",
    "import torch\n",
    "from utils.IO_func import read_file_list, load_binary_file, array_to_binary_file, load_Haskins_SSR_data\n",
    "from shutil import copyfile\n",
    "from utils.transforms import Transform_Compose\n",
    "from utils.transforms import FixMissingValues\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3470243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from comet_ml import Experiment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a438e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
    "\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "        except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_feats = n_feats//2\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2) # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02211d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterMeter(object):\n",
    "    \"\"\"keeps track of total iterations\"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.val += 1\n",
    "\n",
    "    def get(self):\n",
    "        return self.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "c1e2f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "conf_dir = 'conf/SSR_conf.yaml'\n",
    "buff_dir = 'current_exp'\n",
    "\n",
    "config = yaml.load(open(conf_dir, 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "data_path = os.path.join(buff_dir, 'data_CV')\n",
    "SPK_list = ['M01']\n",
    "\n",
    "for test_SPK in SPK_list:\n",
    "    data_path_SPK = os.path.join(data_path, test_SPK)\n",
    "\n",
    "    tr = open(os.path.join(data_path_SPK, 'train_data.pkl'), 'rb') \n",
    "    va = open(os.path.join(data_path_SPK, 'valid_data.pkl'), 'rb')        \n",
    "    train_dataset, valid_dataset = pickle.load(tr), pickle.load(va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f513f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GreedyDecoder(output, labels, label_lengths, blank_label=40, collapse_repeated=True):\n",
    "    \n",
    "    from utils.database import PhoneTransform\n",
    "\n",
    "    text_transform = PhoneTransform()\n",
    "\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "\n",
    "        decode = []\n",
    "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        decodes.append(text_transform.int_to_text(decode))\n",
    "    return decodes, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "42e01d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(data, transforms = None):\n",
    "    ema = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    \n",
    "    for file_id, x, y in data:\n",
    "        if transforms is not None:\n",
    "            x = transforms(x)\n",
    "\n",
    "        ema.append(torch.FloatTensor(x))\n",
    "        labels.append(y)\n",
    "        input_lengths.append(x.shape[0] // 2)\n",
    "        label_lengths.append(len(y))\n",
    "        \n",
    "    ema = torch.nn.utils.rnn.pad_sequence(ema, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)        \n",
    "    \n",
    "    return file_id, ema, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "9dda2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-4\n",
    "batch_size=20\n",
    "epochs=80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "305446c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"n_cnn_layers\": 3,\n",
    "    \"n_rnn_layers\": 2,\n",
    "    \"rnn_dim\": 512,\n",
    "    \"n_class\": 41,\n",
    "    \"n_feats\": 18,\n",
    "    \"stride\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs\n",
    "}\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                            batch_size=8,\n",
    "                            shuffle=True,\n",
    "                            collate_fn=lambda x: data_processing(x, None))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=lambda x: data_processing(x, None))\n",
    "\n",
    "model = SpeechRecognitionModel(\n",
    "    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "#print(model)\n",
    "#print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "criterion = nn.CTCLoss(blank=40).to(device)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                        steps_per_epoch=int(len(train_loader)),\n",
    "                                        epochs=hparams['epochs'],\n",
    "                                        anneal_strategy='linear')\n",
    "\n",
    "iter_meter = IterMeter()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "3a79943b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1414 (0%)]\tLoss: 12.373411\n",
      "Train Epoch: 0 [800/1414 (56%)]\tLoss: 5.254490\n",
      "Train Epoch: 1 [0/1414 (0%)]\tLoss: 3.753989\n",
      "Train Epoch: 1 [800/1414 (56%)]\tLoss: 3.558959\n",
      "Train Epoch: 2 [0/1414 (0%)]\tLoss: 3.505859\n",
      "Train Epoch: 2 [800/1414 (56%)]\tLoss: 3.480398\n",
      "Train Epoch: 3 [0/1414 (0%)]\tLoss: 3.388424\n",
      "Train Epoch: 3 [800/1414 (56%)]\tLoss: 3.356392\n",
      "Train Epoch: 4 [0/1414 (0%)]\tLoss: 3.265356\n",
      "Train Epoch: 4 [800/1414 (56%)]\tLoss: 3.214241\n",
      "Train Epoch: 5 [0/1414 (0%)]\tLoss: 3.234513\n",
      "Train Epoch: 5 [800/1414 (56%)]\tLoss: 3.135473\n",
      "Train Epoch: 6 [0/1414 (0%)]\tLoss: 3.207198\n",
      "Train Epoch: 6 [800/1414 (56%)]\tLoss: 3.174289\n",
      "Train Epoch: 7 [0/1414 (0%)]\tLoss: 3.068218\n",
      "Train Epoch: 7 [800/1414 (56%)]\tLoss: 3.118841\n",
      "Train Epoch: 8 [0/1414 (0%)]\tLoss: 3.161213\n",
      "Train Epoch: 8 [800/1414 (56%)]\tLoss: 3.125982\n",
      "Train Epoch: 9 [0/1414 (0%)]\tLoss: 3.106795\n",
      "Train Epoch: 9 [800/1414 (56%)]\tLoss: 3.165309\n",
      "Train Epoch: 10 [0/1414 (0%)]\tLoss: 3.190001\n",
      "Train Epoch: 10 [800/1414 (56%)]\tLoss: 3.112154\n",
      "Train Epoch: 11 [0/1414 (0%)]\tLoss: 3.087207\n",
      "Train Epoch: 11 [800/1414 (56%)]\tLoss: 3.083613\n",
      "Train Epoch: 12 [0/1414 (0%)]\tLoss: 3.262925\n",
      "Train Epoch: 12 [800/1414 (56%)]\tLoss: 3.061615\n",
      "Train Epoch: 13 [0/1414 (0%)]\tLoss: 3.170373\n",
      "Train Epoch: 13 [800/1414 (56%)]\tLoss: 3.157439\n",
      "Train Epoch: 14 [0/1414 (0%)]\tLoss: 3.131597\n",
      "Train Epoch: 14 [800/1414 (56%)]\tLoss: 3.076450\n",
      "Train Epoch: 15 [0/1414 (0%)]\tLoss: 3.159627\n",
      "Train Epoch: 15 [800/1414 (56%)]\tLoss: 3.015952\n",
      "Train Epoch: 16 [0/1414 (0%)]\tLoss: 3.057293\n",
      "Train Epoch: 16 [800/1414 (56%)]\tLoss: 3.096580\n",
      "Train Epoch: 17 [0/1414 (0%)]\tLoss: 3.062587\n",
      "Train Epoch: 17 [800/1414 (56%)]\tLoss: 3.104439\n",
      "Train Epoch: 18 [0/1414 (0%)]\tLoss: 2.980628\n",
      "Train Epoch: 18 [800/1414 (56%)]\tLoss: 3.045046\n",
      "Train Epoch: 19 [0/1414 (0%)]\tLoss: 3.078264\n",
      "Train Epoch: 19 [800/1414 (56%)]\tLoss: 2.968885\n",
      "Train Epoch: 20 [0/1414 (0%)]\tLoss: 2.976642\n",
      "Train Epoch: 20 [800/1414 (56%)]\tLoss: 2.903591\n",
      "Train Epoch: 21 [0/1414 (0%)]\tLoss: 3.046015\n",
      "Train Epoch: 21 [800/1414 (56%)]\tLoss: 2.833242\n",
      "Train Epoch: 22 [0/1414 (0%)]\tLoss: 2.869990\n",
      "Train Epoch: 22 [800/1414 (56%)]\tLoss: 2.879193\n",
      "Train Epoch: 23 [0/1414 (0%)]\tLoss: 2.849992\n",
      "Train Epoch: 23 [800/1414 (56%)]\tLoss: 2.847155\n",
      "Train Epoch: 24 [0/1414 (0%)]\tLoss: 2.608338\n",
      "Train Epoch: 24 [800/1414 (56%)]\tLoss: 2.672732\n",
      "Train Epoch: 25 [0/1414 (0%)]\tLoss: 2.573351\n",
      "Train Epoch: 25 [800/1414 (56%)]\tLoss: 2.545711\n",
      "Train Epoch: 26 [0/1414 (0%)]\tLoss: 2.389841\n",
      "Train Epoch: 26 [800/1414 (56%)]\tLoss: 2.445835\n",
      "Train Epoch: 27 [0/1414 (0%)]\tLoss: 2.610955\n",
      "Train Epoch: 27 [800/1414 (56%)]\tLoss: 2.383926\n",
      "Train Epoch: 28 [0/1414 (0%)]\tLoss: 2.428197\n",
      "Train Epoch: 28 [800/1414 (56%)]\tLoss: 2.438458\n",
      "Train Epoch: 29 [0/1414 (0%)]\tLoss: 2.429756\n",
      "Train Epoch: 29 [800/1414 (56%)]\tLoss: 2.427739\n",
      "Train Epoch: 30 [0/1414 (0%)]\tLoss: 2.251145\n",
      "Train Epoch: 30 [800/1414 (56%)]\tLoss: 2.238456\n",
      "Train Epoch: 31 [0/1414 (0%)]\tLoss: 2.491165\n",
      "Train Epoch: 31 [800/1414 (56%)]\tLoss: 2.384854\n",
      "Train Epoch: 32 [0/1414 (0%)]\tLoss: 2.410069\n",
      "Train Epoch: 32 [800/1414 (56%)]\tLoss: 2.171732\n",
      "Train Epoch: 33 [0/1414 (0%)]\tLoss: 2.076237\n",
      "Train Epoch: 33 [800/1414 (56%)]\tLoss: 2.169632\n",
      "Train Epoch: 34 [0/1414 (0%)]\tLoss: 1.935345\n",
      "Train Epoch: 34 [800/1414 (56%)]\tLoss: 2.004971\n",
      "Train Epoch: 35 [0/1414 (0%)]\tLoss: 2.068117\n",
      "Train Epoch: 35 [800/1414 (56%)]\tLoss: 2.159053\n",
      "Train Epoch: 36 [0/1414 (0%)]\tLoss: 1.976087\n",
      "Train Epoch: 36 [800/1414 (56%)]\tLoss: 2.272541\n",
      "Train Epoch: 37 [0/1414 (0%)]\tLoss: 1.905970\n",
      "Train Epoch: 37 [800/1414 (56%)]\tLoss: 1.880291\n",
      "Train Epoch: 38 [0/1414 (0%)]\tLoss: 2.012805\n",
      "Train Epoch: 38 [800/1414 (56%)]\tLoss: 1.980993\n",
      "Train Epoch: 39 [0/1414 (0%)]\tLoss: 1.857891\n",
      "Train Epoch: 39 [800/1414 (56%)]\tLoss: 1.849230\n",
      "Train Epoch: 40 [0/1414 (0%)]\tLoss: 1.774298\n",
      "Train Epoch: 40 [800/1414 (56%)]\tLoss: 1.988872\n",
      "Train Epoch: 41 [0/1414 (0%)]\tLoss: 1.835622\n",
      "Train Epoch: 41 [800/1414 (56%)]\tLoss: 1.947238\n",
      "Train Epoch: 42 [0/1414 (0%)]\tLoss: 1.830929\n",
      "Train Epoch: 42 [800/1414 (56%)]\tLoss: 1.879814\n",
      "Train Epoch: 43 [0/1414 (0%)]\tLoss: 1.826802\n",
      "Train Epoch: 43 [800/1414 (56%)]\tLoss: 1.860892\n",
      "Train Epoch: 44 [0/1414 (0%)]\tLoss: 1.853731\n",
      "Train Epoch: 44 [800/1414 (56%)]\tLoss: 1.953518\n",
      "Train Epoch: 45 [0/1414 (0%)]\tLoss: 1.652323\n",
      "Train Epoch: 45 [800/1414 (56%)]\tLoss: 1.705633\n",
      "Train Epoch: 46 [0/1414 (0%)]\tLoss: 1.753311\n",
      "Train Epoch: 46 [800/1414 (56%)]\tLoss: 1.536513\n",
      "Train Epoch: 47 [0/1414 (0%)]\tLoss: 1.556975\n",
      "Train Epoch: 47 [800/1414 (56%)]\tLoss: 1.861390\n",
      "Train Epoch: 48 [0/1414 (0%)]\tLoss: 1.702999\n",
      "Train Epoch: 48 [800/1414 (56%)]\tLoss: 1.830201\n",
      "Train Epoch: 49 [0/1414 (0%)]\tLoss: 1.567432\n",
      "Train Epoch: 49 [800/1414 (56%)]\tLoss: 1.733768\n",
      "Train Epoch: 50 [0/1414 (0%)]\tLoss: 1.560528\n",
      "Train Epoch: 50 [800/1414 (56%)]\tLoss: 1.606037\n",
      "Train Epoch: 51 [0/1414 (0%)]\tLoss: 1.629653\n",
      "Train Epoch: 51 [800/1414 (56%)]\tLoss: 1.532051\n",
      "Train Epoch: 52 [0/1414 (0%)]\tLoss: 1.570135\n",
      "Train Epoch: 52 [800/1414 (56%)]\tLoss: 1.563335\n",
      "Train Epoch: 53 [0/1414 (0%)]\tLoss: 1.590317\n",
      "Train Epoch: 53 [800/1414 (56%)]\tLoss: 1.560689\n",
      "Train Epoch: 54 [0/1414 (0%)]\tLoss: 1.466929\n",
      "Train Epoch: 54 [800/1414 (56%)]\tLoss: 1.492506\n",
      "Train Epoch: 55 [0/1414 (0%)]\tLoss: 1.570204\n",
      "Train Epoch: 55 [800/1414 (56%)]\tLoss: 1.532225\n",
      "Train Epoch: 56 [0/1414 (0%)]\tLoss: 1.521206\n",
      "Train Epoch: 56 [800/1414 (56%)]\tLoss: 1.319271\n",
      "Train Epoch: 57 [0/1414 (0%)]\tLoss: 1.472029\n",
      "Train Epoch: 57 [800/1414 (56%)]\tLoss: 1.575686\n",
      "Train Epoch: 58 [0/1414 (0%)]\tLoss: 1.566647\n",
      "Train Epoch: 58 [800/1414 (56%)]\tLoss: 1.445696\n",
      "Train Epoch: 59 [0/1414 (0%)]\tLoss: 1.490126\n",
      "Train Epoch: 59 [800/1414 (56%)]\tLoss: 1.373570\n",
      "Train Epoch: 60 [0/1414 (0%)]\tLoss: 1.432537\n",
      "Train Epoch: 60 [800/1414 (56%)]\tLoss: 1.336078\n",
      "Train Epoch: 61 [0/1414 (0%)]\tLoss: 1.223550\n",
      "Train Epoch: 61 [800/1414 (56%)]\tLoss: 1.306957\n",
      "Train Epoch: 62 [0/1414 (0%)]\tLoss: 1.479208\n",
      "Train Epoch: 62 [800/1414 (56%)]\tLoss: 1.294498\n",
      "Train Epoch: 63 [0/1414 (0%)]\tLoss: 1.223657\n",
      "Train Epoch: 63 [800/1414 (56%)]\tLoss: 1.239658\n",
      "Train Epoch: 64 [0/1414 (0%)]\tLoss: 1.339410\n",
      "Train Epoch: 64 [800/1414 (56%)]\tLoss: 1.484021\n",
      "Train Epoch: 65 [0/1414 (0%)]\tLoss: 1.193460\n",
      "Train Epoch: 65 [800/1414 (56%)]\tLoss: 1.318626\n",
      "Train Epoch: 66 [0/1414 (0%)]\tLoss: 1.287007\n",
      "Train Epoch: 66 [800/1414 (56%)]\tLoss: 1.210312\n",
      "Train Epoch: 67 [0/1414 (0%)]\tLoss: 1.284159\n",
      "Train Epoch: 67 [800/1414 (56%)]\tLoss: 1.300253\n",
      "Train Epoch: 68 [0/1414 (0%)]\tLoss: 1.296670\n",
      "Train Epoch: 68 [800/1414 (56%)]\tLoss: 1.172038\n",
      "Train Epoch: 69 [0/1414 (0%)]\tLoss: 1.231727\n",
      "Train Epoch: 69 [800/1414 (56%)]\tLoss: 1.240351\n",
      "Train Epoch: 70 [0/1414 (0%)]\tLoss: 1.061740\n",
      "Train Epoch: 70 [800/1414 (56%)]\tLoss: 1.355280\n",
      "Train Epoch: 71 [0/1414 (0%)]\tLoss: 1.284258\n",
      "Train Epoch: 71 [800/1414 (56%)]\tLoss: 1.105432\n",
      "Train Epoch: 72 [0/1414 (0%)]\tLoss: 1.174916\n",
      "Train Epoch: 72 [800/1414 (56%)]\tLoss: 1.042509\n",
      "Train Epoch: 73 [0/1414 (0%)]\tLoss: 1.193409\n",
      "Train Epoch: 73 [800/1414 (56%)]\tLoss: 1.105926\n",
      "Train Epoch: 74 [0/1414 (0%)]\tLoss: 1.223366\n",
      "Train Epoch: 74 [800/1414 (56%)]\tLoss: 1.210355\n",
      "Train Epoch: 75 [0/1414 (0%)]\tLoss: 1.019672\n",
      "Train Epoch: 75 [800/1414 (56%)]\tLoss: 1.224851\n",
      "Train Epoch: 76 [0/1414 (0%)]\tLoss: 1.156631\n",
      "Train Epoch: 76 [800/1414 (56%)]\tLoss: 1.067380\n",
      "Train Epoch: 77 [0/1414 (0%)]\tLoss: 1.285587\n",
      "Train Epoch: 77 [800/1414 (56%)]\tLoss: 1.211802\n",
      "Train Epoch: 78 [0/1414 (0%)]\tLoss: 1.268632\n",
      "Train Epoch: 78 [800/1414 (56%)]\tLoss: 1.255560\n",
      "Train Epoch: 79 [0/1414 (0%)]\tLoss: 1.123684\n",
      "Train Epoch: 79 [800/1414 (56%)]\tLoss: 1.021994\n"
     ]
    }
   ],
   "source": [
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, _data in enumerate(train_loader):\n",
    "            file_id, ema, labels, input_lengths, label_lengths = _data \n",
    "            ema, labels = ema.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(ema)  # (batch, time, n_class)\n",
    "\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            iter_meter.step()\n",
    "            if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(ema), data_len,\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "d41226d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n",
      "0.8303094983991463\n"
     ]
    }
   ],
   "source": [
    "    model.eval()\n",
    "    pred = []\n",
    "    label = []\n",
    "\n",
    "    from jiwer import wer\n",
    "\n",
    "    for batch_idx, _data in enumerate(test_loader):\n",
    "        fid, ema, labels, input_lengths, label_lengths = _data \n",
    "        ema, labels = ema.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(ema)  # (batch, time, n_class)\n",
    "\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "      #  test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "        decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "\n",
    "     #   print(decoded_preds)\n",
    "     #   print(decoded_targets)\n",
    "\n",
    "        pred.append(' '.join(decoded_preds[0]))\n",
    "        label.append(' '.join(decoded_targets[0]))\n",
    "    \n",
    "print(len(pred))\n",
    "print(len(label))\n",
    "\n",
    "error = wer(pred, label)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbecfb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "        for epoch in range(epochs):\n",
    "            for batch_idx, _data in enumerate(train_loader):\n",
    "                file_id, ema, labels, input_lengths, label_lengths = _data \n",
    "                ema, labels = ema.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output = model(ema)  # (batch, time, n_class)\n",
    "\n",
    "                output = F.log_softmax(output, dim=2)\n",
    "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                iter_meter.step()\n",
    "                if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(ema), data_len, 100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "            for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "94f6ab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP DH AH S   R B ER R AH Z AE M B ER   D ER M DH AH IY SP\n",
      "SP DH AH S AO L T B R IY Z K EY M AH K R AO S F ER M DH AH S IY SP\n",
      "#####################\n",
      "SP DH AH S   R M ER N Z S K IY P AH ER R AH Z ER M AH S IH SP\n",
      "SP DH AH S AO L T B R IY Z K EY M AH K R AO S F ER M DH AH S IY SP\n",
      "#####################\n",
      "SP AH R IH M IH Z AH L N AE N M   R K N M\n",
      "SP DH AH G ER L AE T DH AH B UW TH S OW L D F IH F T IY B   N D Z SP\n",
      "#####################\n",
      "SP DH AH L AE D DH AH M UW S OW N EH N IY B   N S SP\n",
      "SP DH AH G ER L AE T DH AH B UW TH S OW L D F IH F T IY B   N D Z SP\n",
      "#####################\n",
      "SP   L EY N DH AH M P UW OW EH IH B   Z SP\n",
      "SP DH AH G ER L AE T DH AH B UW TH S OW L D F IH F T IY B   N D Z SP\n",
      "#####################\n",
      "SP DH AH S M AH P   M   T L OY AH Z S AH L SP\n",
      "SP DH AH S M AO L P AH P N AO D AH SHH OW L AH N DH AH S   K SP\n",
      "#####################\n",
      "SP DH AH S M AO   R B M ER   N W S AO D\n",
      "SP DH AH S M AO L P AH P N AO D AH SHH OW L AH N DH AH S   K SP\n",
      "#####################\n",
      "SP DH AH S M   L M P   P AH D AH   N T AH W L IH AH S   SP\n",
      "SP DH AH S M AO L P AH P SP N AO D AH SHH OW L AH N DH AH S   K SP\n",
      "#####################\n",
      "SP DH AH IH S W R IH ER S T AH D   DH AH M EH N T SP\n",
      "SP DH AH F IH SH T W IH S T AH D AH N D T ER N D   N DH AH B EH N T SHH UH K SP\n",
      "#####################\n",
      "SP DH AH IH S R W EH S IH AH T ER R D AH N DH AH B EH N K SP\n",
      "SP DH AH F IH SH SP T W IH S T AH D AE N D T ER N D   N DH AH B EH N T SHH UH K SP\n",
      "#####################\n",
      "SP DH AH IH AH S W AH EH T AY SP B EH N T SP\n",
      "SP DH AH F IH SH T W IH S T AH D AE N D T ER N D   N DH AH B EH N T SHH UH K SP\n",
      "#####################\n",
      "SP M R EH P EY N S AH S S OW L M N L   AE EH T SP\n",
      "SP P R EH S DH AH P AE N T S AH N D S OW AH B AH T AH N   N DH AH V EH S T SP\n",
      "#####################\n",
      "SP B R EH T AH S AH P AE S EH N D S OW M AO R AY F AE S SP\n",
      "SP P R EH S DH AH P AE N T S AE N D S OW AH B AH T AH N   N DH AH V EH S T SP\n",
      "#####################\n",
      "SP P R S DH AH P AE N D S IH S AH M AH S F AE S S SP\n",
      "SP P R EH S DH AH P AE N T S AE N D S OW AH B AH T AH N   N DH AH V EH S T SP\n",
      "#####################\n",
      "SP DH AH S W AO EH TH   R SH AO R D B ER N SP\n",
      "SP DH AH S W   N D AY V W AH Z F   R SH AO R T AH V P ER F EH K T SP\n",
      "#####################\n",
      "SP DH AH S W AO L D D AY F ER AO R DH AH AO R EH S SP\n",
      "SP DH AH S W   N D AY V W AH Z F   R SH AO R T AH V P ER F IH K T SP\n",
      "#####################\n",
      "SP DH AH S W AH AY UW Z F   R SH AO R B R F N SP\n",
      "SP DH AH S W   N D AY V W AH Z F   R SH AO R T AH V P ER F EH K T SP\n",
      "#####################\n",
      "SP DH AH M P AH R AH N DH IY K AH S T   N AH N DH EH R P L SP\n",
      "SP DH AH B Y UW T IY AH V DH IY V Y UW S T AH N D DH AH Y AH NG B OY SP\n",
      "#####################\n",
      "SP DH AH P IH R IY IH Z S T AH AH DH N D B L T SP\n",
      "SP DH AH B Y UW T IY AH V DH AH V Y UW S T AH N D DH IY Y AH NG B OY SP\n",
      "#####################\n",
      "SP DH AH P IY R IY ER Z Z T AW N T AH DH   P EY SP\n",
      "SP DH AH B Y UW T IY AH V DH AH V Y UW S T AH N D DH IY Y AH NG B OY SP\n",
      "#####################\n",
      "SP T UW P L V Z W EY P IH DH AH CH T EY K D\n",
      "SP T UW B L UW F IH SH S W AE M AH N DH AH T AE NG K SP\n",
      "#####################\n",
      "SP T UW P L AH IH Z W IY B AH IH DH AH EH SP\n",
      "SP T UW B L UW F IH SH S W AE M AH N DH AH T AE NG K SP\n",
      "#####################\n",
      "SP T UW P L OW IH Z W IY M IH IH SP\n",
      "SP T UW B L UW F IH SH S W AE M AH N DH AH T AE NG K SP\n",
      "#####################\n",
      "SP AH M ER W AH Z AH L IY Z S EH T SP\n",
      "SP SHH ER P ER S W AH Z F UH L AH V Y UW S L AH S T R AE SH SP\n",
      "#####################\n",
      "SP AH P   R Z W AH Z AH L IH S IH SH\n",
      "SP SHH ER P ER S W AH Z F UH L AH V Y UW S L AH S T R AE SH SP\n",
      "#####################\n",
      "SP P S W AH Z V F   L Z IH S\n",
      "SP SHH ER P ER S W AH Z F UH L AH V Y UW S L AH S T R AE SH SP\n",
      "#####################\n",
      "SP DH AH K OW W AH IY DH ER DH AH R AY D ER\n",
      "SP DH AH K OW L T R IH R D AH N D TH R UW DH AH T AO L R AY D ER SP\n",
      "#####################\n",
      "SP DH AH K OW L D R IH DH AH Z W R DH AH T AO L R AY D SP\n",
      "SP DH AH K OW L T R IH R D AH N D TH R UW DH AH T AO L R AY D ER SP\n",
      "#####################\n",
      "SP AH Y K OW L D W R IH L D IY N W Z DH AH AO L R AY ER SP\n",
      "SP DH AH K OW L T R IH R D AE N D TH R UW DH AH T AO L R AY D ER SP\n",
      "#####################\n",
      "SP DH AH S T ER W R AY N EY L AH Z IY P AO R IH L\n",
      "SP IH T S N OW D R EY N D AH N D SHH EY L D DH AH S EY M M AO R N IH NG SP\n",
      "#####################\n",
      "SP SHH IY D W R EY N D AE N L EY AH S IY B R SP\n",
      "SP IH T S N OW D R EY N D AE N D SHH EY L D DH AH S EY M M AO R N IH NG SP\n",
      "#####################\n",
      "SP DH AH S T D R AY N AH N T EY F S AE M IH NG K AE\n",
      "SP IH T S N OW D R EY N D AE N D SHH EY L D SP DH AH S EY M M AO R N IH NG SP\n",
      "#####################\n",
      "SP W R IY F ER Z AW AE D AE D T W L EY SH AH N SP\n",
      "SP R IY D V ER S AW T L AW D F ER P L EH ZH ER SP\n",
      "#####################\n",
      "SP IY N S AW L D AE N D ER B L EY S M SP\n",
      "SP R IY D V ER S AW T L AW D F ER P L EH ZH ER SP\n",
      "#####################\n",
      "SP W AY F R Z S EH T N D T F ER AH B L EY S Z SP\n",
      "SP R IY D V ER S AW T L AW D F ER P L EH ZH ER SP\n",
      "#####################\n",
      "SP IY DH AH L OW D AH L AE SH L SP\n",
      "SP SHH OY S T DH AH L OW D T IH Y UH R L EH F T SH OW L D ER SP\n",
      "#####################\n",
      "SP EY T AH N Z DH AH L AH L D ER L AE N D SH OW L TH\n",
      "SP SHH OY S T DH AH L OW D T IH Y UH R L EH F T SH OW L D ER SP\n",
      "#####################\n",
      "SP IY S AH V L AO L L D Y ER AE N D SH OW L SP\n",
      "SP SHH OY S T DH AH L OW D T IH Y UH R L EH F T SH OW L D ER SP\n",
      "#####################\n",
      "SP T EY DH AH P   R M AE T AH N AH R IY S T L IY SP\n",
      "SP T EY K DH AH W AY N D IH NG P AE TH T AH R IY CH SP DH AH L EY K SP\n",
      "#####################\n",
      "SP T EY T DH AH AY D AH M AE AH DH W IH AH SP\n",
      "SP T EY K DH AH W AY N D IH NG P AE TH T IH R IY CH SP DH AH L EY K SP\n",
      "#####################\n",
      "SP T EY K DH AH W AY AH P AE DH AH R IY AH L IY SP\n",
      "SP T EY K DH AH W AY N D IH NG P AE TH T IH R IY CH SP DH AH L EY K SP\n",
      "#####################\n",
      "SP N L OW S IH S EY AY S AH V AE S R F SP\n",
      "SP N OW T K L OW S L IY DH AH S AY Z AH V DH AH G AE S T AE NG K SP\n",
      "#####################\n",
      "SP OW T K L Z L EY D DH AH S AY S AH V DH IY AE S T SP\n",
      "SP N OW T K L OW S L IY DH AH S AY Z AH V DH AH G AE S T AE NG K SP\n",
      "#####################\n",
      "SP OW D K Z L   DH AH S AY S ER DH IY S T EY SP\n",
      "SP N OW T K L OW S L IY DH AH S AY Z AH V DH AH G AE S T AE NG K SP\n",
      "#####################\n",
      "SP W AY B AH R IY T   F S T N IY DH EY S S N\n",
      "SP W AY P DH AH G R IY S AO F SHH AH Z D ER T IY F EY S SP\n",
      "#####################\n",
      "SP W   P AH R IH S   L EH N D AH S AH N IY AH F EH S M\n",
      "SP W AY P DH AH G R IY S AO F SHH IH Z D ER T IY F EY S SP\n",
      "#####################\n",
      "SP P M AH R IY Z   AH S R IY L EY S Z\n",
      "SP W AY P DH AH G R IY S AO F SHH IH Z D ER T IY SP F EY S SP\n",
      "#####################\n",
      "SP B EY AE T AH OW P F R Y L   AE T SP\n",
      "SP M EH N D DH AH K OW T B AH F AO R Y UW G OW AW T SP\n",
      "#####################\n",
      "SP P EH N T AH K AE OW D P AH F AO R Y AH V AH AW SP\n",
      "SP M EH N D DH AH K OW T B AH F AO R Y UW G OW AW T SP\n",
      "#####################\n",
      "SP P AE IH NG AE OW B P AH AO R D\n",
      "SP M EH N D DH AH K OW T B AH F AO R Y UW G OW AW T SP\n",
      "#####################\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pred)):\n",
    "    print(pred[i])\n",
    "    print(label[i])\n",
    "    print('#####################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df763f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc990e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
